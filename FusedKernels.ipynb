{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62bca490-d730-4aad-9cd9-1a7551ea2eae",
   "metadata": {},
   "source": [
    "# Fused Kernels - What started as exploring DLRM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8643efce-dc8f-43a2-b28c-27a7c41f66f3",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Abstract: With focus on performance to get the most out of hardware, fusing of kernels has been a popular technique. At times, researchers/practitioners will re-write their code in native cuda or cpu kernels to get optimal performance, but projects such as torch.compile aim to make this simpler. Talk will focus on generating fused kernels and how to leverage torch.compile to be able to do that. We will shift a bit from all LLM talk and look into recommendation algorithms as big deep learning/AI systems. In the process, we will work on creating fused kernels (triton and cuda) with the help of torch.compile. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43570ae8-5020-480c-84cb-4004722a51f4",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be61622a-2bcb-4f93-910b-639e10d0b69f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Code and other artifacts\n",
    "\n",
    "- Lecture code: https://github.com/kapilsh/cuda-mode-lecture\n",
    "- How to open chrome trace: chrome://tracing\n",
    "- DLRM Blog Post: https://ai.meta.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/\n",
    "- DLRM Paper: https://arxiv.org/pdf/1906.00091\n",
    "- DLRM github repo: https://github.com/facebookresearch/dlrm\n",
    "- Criteo Dataset: https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/\n",
    "\n",
    "\n",
    "## DLRM (Deep Learning Recommendation Model)\n",
    "\n",
    "### MODEL ARCHITECTURE \n",
    "\n",
    "![DLRM Model](./data/dlrm_model.png)\n",
    "\n",
    "### System Constrants\n",
    "\n",
    "![System Constraints](./data/66324023_2056206621174067_2937830378620059648_n.gif)\n",
    "\n",
    "### Criteo Dataset\n",
    "\n",
    "- Training dataset with 24 days of ad display and click data (positive: clicked and negatives: non-clicked)\n",
    "- 13 features taking integer values (mostly count features)\n",
    "- 26 anonymized categorical features\n",
    "- Corresponding Kaggle competition: https://www.kaggle.com/c/criteo-display-ad-challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76eb45b-1ee6-4257-98a6-5185f57987a6",
   "metadata": {},
   "source": [
    "# Exploring DLRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8b063e1-6f04-4665-ac70-8508641d37b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Mapping, List, Dict, Union\n",
    "\n",
    "import click\n",
    "import torch\n",
    "import torch._dynamo\n",
    "from loguru import logger\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from criteo_dataset import CriteoParquetDataset\n",
    "from model import DenseArch, read_metadata, SparseArch, DenseSparseInteractionLayer, PredictionLayer, Parameters, DLRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1069bd19-a050-4c9b-8d1f-051e3bd7e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./data/sample_criteo_data.parquet\"\n",
    "metadata_path = \"./data/sample_criteo_metadata.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42b72f15-8b72-4345-8d2f-a71ad4b8f01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 21:10:14.301 | INFO     | __main__:<module>:1 - Reading the parquet file ./data/sample_criteo_data.parquet...\n",
      "2024-05-05 21:10:14.302 | INFO     | __main__:<module>:2 - Reading the metadata file ./data/sample_criteo_metadata.json...\n",
      "2024-05-05 21:10:15.447 | INFO     | __main__:<module>:7 - Labels size: torch.Size([2])\n",
      "2024-05-05 21:10:15.448 | INFO     | __main__:<module>:8 - Dense size: torch.Size([2, 13])\n",
      "2024-05-05 21:10:15.448 | INFO     | __main__:<module>:9 - Sparse size: torch.Size([2, 26])\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Reading the parquet file {}...\".format(file_path))\n",
    "logger.info(\"Reading the metadata file {}...\".format(metadata_path))\n",
    "\n",
    "dataset = CriteoParquetDataset(file_path)\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=False)\n",
    "labels, dense, sparse = next(iter(data_loader))\n",
    "logger.info(\"Labels size: {}\".format(labels.size()))\n",
    "logger.info(\"Dense size: {}\".format(dense.size()))\n",
    "logger.info(\"Sparse size: {}\".format(sparse.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "884bc4d3-a489-45a1-b269-141db29a3b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.0000e+00, 1.1000e+02, 0.0000e+00, 1.6000e+01, 0.0000e+00, 1.0000e+00,\n",
       "         0.0000e+00, 1.4000e+01, 7.0000e+00, 1.0000e+00, 0.0000e+00, 3.0600e+02,\n",
       "         0.0000e+00],\n",
       "        [3.2000e+01, 3.0000e+00, 5.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 6.1000e+01, 5.0000e+00, 0.0000e+00, 1.0000e+00, 3.1570e+03,\n",
       "         5.0000e+00]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c896c0f1-3773-4af5-bcc4-1aa1b17fb409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1651969401, 3793706328, 2951365679, 2489089999,  951068488, 1875733963,\n",
       "          897624609,  679512323, 1189011366,  771915201,  209470001, 2509774111,\n",
       "           12976055, 3192841527, 2316006604, 1289502458, 3523761834, 3088518074,\n",
       "         2501034507, 3280875304,  351689309,  632402057, 3619814411, 2091868316,\n",
       "          809724924, 3977271069],\n",
       "        [3857972621, 2695561126, 1873417685, 3666490401, 1020698403, 1875733963,\n",
       "         2870406529, 1128426537,  502653268, 2112471209, 1716706404, 2582335015,\n",
       "           12976055, 3192841527, 4089183897, 1289502458, 3523761834, 2716538129,\n",
       "         2501034507, 4273985635, 2737978529, 3370249814,  391309800, 1966410890,\n",
       "         2568167914, 3075991895]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e0476a3-0af3-4de7-985e-76a2f13d8da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 21:10:16.415 | INFO     | __main__:<module>:7 - Dense out size: torch.Size([2, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  -8.9006,   40.5694,   29.5478,   -8.9894,  -15.8872,   40.5282,\n",
       "          -45.3048,  -10.0197,  -34.3252,   34.9440,  -20.3214,  -23.4073,\n",
       "          -27.0077,   14.4200,   16.2891,   23.7067],\n",
       "        [ -20.1532,  249.8419,  418.2879, -123.5538, -127.3028,  470.2328,\n",
       "         -476.5699,    3.4968, -396.1133,  421.3827, -245.7733, -277.4859,\n",
       "         -231.9042,  209.6555,  157.4375,  275.0896]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dense_mlp_out_size = 16\n",
    "num_dense_features = dense.size()[1]\n",
    "dense_arch = DenseArch(dense_feature_count=num_dense_features,\n",
    "                       dense_hidden_layers_sizes=[32],\n",
    "                       output_size=dense_mlp_out_size)\n",
    "dense_out = dense_arch(dense)\n",
    "logger.info(\"Dense out size: {}\".format(dense_out.size()))\n",
    "dense_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eca531ef-68ee-4773-821a-69518e3fed07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 21:10:18.757 | INFO     | __main__:<module>:11 - Sparse out size: torch.Size([2, 16])\n",
      "2024-05-05 21:10:18.758 | INFO     | __main__:<module>:11 - Sparse out size: torch.Size([2, 16])\n",
      "2024-05-05 21:10:18.758 | INFO     | __main__:<module>:11 - Sparse out size: torch.Size([2, 16])\n",
      "2024-05-05 21:10:18.758 | INFO     | __main__:<module>:11 - Sparse out size: torch.Size([2, 16])\n",
      "2024-05-05 21:10:18.759 | INFO     | __main__:<module>:11 - Sparse out size: torch.Size([2, 16])\n",
      "2024-05-05 21:10:18.759 | INFO     | __main__:<module>:11 - Sparse out size: torch.Size([2, 16])\n",
      "2024-05-05 21:10:18.760 | INFO     | __main__:<module>:11 - Sparse out size: torch.Size([2, 16])\n",
      "2024-05-05 21:10:18.760 | INFO     | __main__:<module>:11 - Sparse out size: torch.Size([2, 16])\n",
      "2024-05-05 21:10:18.761 | INFO     | __main__:<module>:11 - Sparse out size: torch.Size([2, 16])\n",
      "2024-05-05 21:10:18.761 | INFO     | __main__:<module>:11 - Sparse out size: torch.Size([2, 16])\n",
      "2024-05-05 21:10:18.761 | INFO     | __main__:<module>:11 - Sparse out size: torch.Size([2, 16])\n",
      "2024-05-05 21:10:18.761 | INFO     | __main__:<module>:11 - Sparse out size: torch.Size([2, 16])\n",
      "2024-05-05 21:10:18.762 | INFO     | __main__:<module>:11 - Sparse out size: torch.Size([2, 16])\n",
      "2024-05-05 21:10:18.762 | INFO     | __main__:<module>:11 - Sparse out size: torch.Size([2, 16])\n",
      "2024-05-05 21:10:18.763 | INFO     | __main__:<module>:11 - Sparse out size: torch.Size([2, 16])\n",
      "2024-05-05 21:10:18.763 | INFO     | __main__:<module>:11 - Sparse out size: torch.Size([2, 16])\n",
      "2024-05-05 21:10:18.763 | INFO     | __main__:<module>:11 - Sparse out size: torch.Size([2, 16])\n",
      "2024-05-05 21:10:18.764 | INFO     | __main__:<module>:11 - Sparse out size: torch.Size([2, 16])\n",
      "2024-05-05 21:10:18.764 | INFO     | __main__:<module>:11 - Sparse out size: torch.Size([2, 16])\n",
      "2024-05-05 21:10:18.764 | INFO     | __main__:<module>:11 - Sparse out size: torch.Size([2, 16])\n",
      "2024-05-05 21:10:18.764 | INFO     | __main__:<module>:11 - Sparse out size: torch.Size([2, 16])\n",
      "2024-05-05 21:10:18.765 | INFO     | __main__:<module>:11 - Sparse out size: torch.Size([2, 16])\n",
      "2024-05-05 21:10:18.765 | INFO     | __main__:<module>:11 - Sparse out size: torch.Size([2, 16])\n",
      "2024-05-05 21:10:18.765 | INFO     | __main__:<module>:11 - Sparse out size: torch.Size([2, 16])\n",
      "2024-05-05 21:10:18.765 | INFO     | __main__:<module>:11 - Sparse out size: torch.Size([2, 16])\n",
      "2024-05-05 21:10:18.766 | INFO     | __main__:<module>:11 - Sparse out size: torch.Size([2, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3973, -0.4610,  2.0486, -0.2744, -0.3556, -0.8468, -0.5945, -1.5288,\n",
       "          0.1601, -0.1903,  0.1085, -0.4725,  1.2473, -0.8733, -1.9742,  1.7321],\n",
       "        [-0.0477, -1.7535,  0.2312,  0.4713,  0.9088,  1.1122,  1.4918, -1.7666,\n",
       "          0.1965, -0.4317,  1.0522, -3.0231, -1.1296,  0.2273,  0.0119, -0.1556]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = read_metadata(metadata_path)\n",
    "embedding_size = 16\n",
    "embedding_sizes = {fn: embedding_size for fn in metadata.keys()}\n",
    "sparse_mlp_out_size = 16\n",
    "sparse_arch = SparseArch(metadata=metadata,\n",
    "                         embedding_sizes=embedding_sizes)\n",
    "# compiled model hangs on running with inputs\n",
    "# sparse_arch_optim = torch.compile(sparse_arch)\n",
    "sparse_out = sparse_arch(sparse)\n",
    "for v in sparse_out:\n",
    "    logger.info(\"Sparse out size: {}\".format(v.size()))\n",
    "sparse_out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "303ed5c6-d3f4-47b6-8ec4-02b1fd889444",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 21:10:18.983 | INFO     | __main__:<module>:3 - Dense sparse interaction out size: torch.Size([2, 186624])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.9221e+01, -3.6109e+02, -2.6299e+02,  ...,  1.2273e-01,\n",
       "         -2.9036e-02,  5.1416e-03],\n",
       "        [ 4.0615e+02, -5.0351e+03, -8.4298e+03,  ...,  1.0812e+00,\n",
       "         -7.4899e-01,  1.1686e+00]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_sparse_interaction_layer = DenseSparseInteractionLayer()\n",
    "ds_out = dense_sparse_interaction_layer(dense_out, sparse_out)\n",
    "logger.info(\"Dense sparse interaction out size: {}\".format(ds_out.size()))\n",
    "ds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "965bb04a-8025-4398-8924-7e2d7b1e857e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 21:10:21.721 | INFO     | __main__:<module>:5 - Prediction out size: torch.Size([2, 1])\n",
      "2024-05-05 21:10:21.721 | INFO     | __main__:<module>:6 - Prediction out value: tensor([[0.0213],\n",
      "        [0.0000]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "prediction_layer = PredictionLayer(dense_out_size=dense_mlp_out_size,\n",
    "                                   sparse_out_sizes=[sparse_mlp_out_size] * len(metadata),\n",
    "                                   hidden_sizes=[16])\n",
    "pred_out = prediction_layer(ds_out)\n",
    "logger.info(\"Prediction out size: {}\".format(pred_out.size()))\n",
    "logger.info(\"Prediction out value: {}\".format(pred_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fe15e0-15a1-4cc2-8406-a46ebb489c42",
   "metadata": {},
   "source": [
    "# ONNX Model Graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e291be4b-6e30-4ba1-aedb-5b54cba4feeb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model Graph\n",
    "\n",
    "![Model Graph](./data/model_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e1ac03-4806-4a78-ae31-31dd469e932d",
   "metadata": {},
   "source": [
    "# Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031a5bea-c03b-466b-a507-9f3b4497bc7a",
   "metadata": {},
   "source": [
    "### Initial Setup: Simple 2 layered MLP used for each triangle\n",
    "\n",
    "### Baseline\n",
    "\n",
    "> python model_train.py\n",
    "\n",
    "### Initial Distribution - Naive Implementation of index_hash\n",
    "\n",
    "![Initial index](./perf_screenshots/pytorch_profile_initial_index_hash.png)\n",
    "*Pytorch Profiler trace (initial)*\n",
    "\n",
    "---\n",
    "\n",
    "> tensorboard --logdir tb_logs --bind_all\n",
    "\n",
    "![Summary Initial](./perf_screenshots/summary_initial_index_hash.png)\n",
    "*Initial distribution of ops - summary from tensorboard*\n",
    "\n",
    "---\n",
    "\n",
    "### Tensor.item() takes a lot of running time\n",
    "\n",
    "- What's going on - what is _local_scalar_dense and why is item() taking so long?\n",
    "    - https://discuss.pytorch.org/t/tensor-item-takes-a-lot-of-running-time/16683\n",
    "    - https://discuss.pytorch.org/t/calling-loss-item-is-very-slow/99774 \n",
    "\n",
    "> CUDA_LAUNCH_BLOCKING=1 python model_train.py\n",
    "\n",
    "---\n",
    "\n",
    "### After passing `CUDA_LAUNCH_BLOCKING=1`\n",
    "\n",
    "![Summary Initial](./perf_screenshots/summary_initial_cuda_launch_blocking.png)\n",
    "*New distribution of ops after `CUDA_LAUNCH_BLOCKING=1` - summary from tensorboard*\n",
    "\n",
    "---\n",
    "\n",
    "![Initial Index Hash Profile](./perf_screenshots/index_hash_profile_1.png)\n",
    "*Profile initial index hash implementation*\n",
    "\n",
    "![Index Hash After Improvement](./perf_screenshots/improve_index_hash.png)\n",
    "*Profile after improvements*\n",
    "\n",
    "\n",
    "- Index hash seems pretty expensive\n",
    "- Not adaptive to new sparse ids\n",
    "- Can we improve/simplify the hash function\n",
    "- Let's just calculate the modulus hash based on cardinality\n",
    "    - Maybe not representative of data if distribution is non uniform across categories (but that's fine for now) \n",
    "\n",
    "---\n",
    "\n",
    "### Using Modulus Hash\n",
    "\n",
    "![Naive Modulus Hash](./perf_screenshots/naive_modulus_hash.png)\n",
    "*Pytorch Profiler trace for naive modulus hash*\n",
    "\n",
    "- Let's check the time it took for each of the previous versions\n",
    "- Wall time down from 48ms -> 5ms\n",
    "\n",
    "![Optimized Modulus Hash](./perf_screenshots/optimized_modulus_hash.png)\n",
    "*Pytorch Profiler trace for optimized modulus hash*\n",
    "\n",
    "- Down to 3.48ms\n",
    "\n",
    "[ ] TODO: Add summary table of DLRM wall time\n",
    "\n",
    "---\n",
    "\n",
    "Based on the profile, what could be the next thing to look at?\n",
    "\n",
    "- index_select?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34436457-a6c9-4489-bc64-b487266c2ca4",
   "metadata": {},
   "source": [
    "# torch.compile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbdc60a-5d38-4fd8-8498-3f9a7f283220",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##  torch.compile DLRM\n",
    "\n",
    "> TORCH_COMPILE_DEBUG_DIR=/home/ksharma/logs TORCH_LOGS=recompiles,+dynamo,inductor,guards,graph_breaks python model.py\n",
    "\n",
    "> CUDA_LAUNCH_BLOCKING=1 python model_train.py\n",
    "\n",
    "- GPU utilization goes up\n",
    "- memory footprint goes down\n",
    "\n",
    "## Memory Footprint\n",
    "\n",
    "### Pre `torch.compile`\n",
    "![Pre torch.compile](./perf_screenshots/pre_torch_compile_initial.png)\n",
    "\n",
    "### Post `torch.compile`\n",
    "![Post torch.compile](./perf_screenshots/post_torch_compile_initial.png)\n",
    "\n",
    "---\n",
    "\n",
    "### Chrome Trace after `torch.compile`\n",
    "![Chrome Trace](./perf_screenshots/pytorch_profile_torch_compile.png)\n",
    "*Pytorch Profile Trace after `torch.compile`\n",
    "\n",
    "---\n",
    "\n",
    "### Let's look deeper into what's going on\n",
    "\n",
    "![torch compile triton kernels](./perf_screenshots/torch_compile_triton_kernels.png)\n",
    "*Custom triton kernel scheduled on the cuda stream*\n",
    "\n",
    "### Increase complexity\n",
    "\n",
    "Source: https://ai.meta.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/\n",
    "\n",
    "```shell\n",
    "python dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=\"13-512-256-64-16\" --arch-mlp-top=\"512-256-1\" --data-generation=dataset --data-set=kaggle --processed-data-file=./input/kaggle_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=1024 --print-time\n",
    "```\n",
    "\n",
    "### Let's change the model architecture\n",
    "\n",
    "- --arch-mlp-bot=\"13-512-256-64-16\"\n",
    "- --arch-mlp-top=\"512-256-1\"\n",
    "\n",
    "### Eager view\n",
    "\n",
    "![Full Model Eager View](./perf_screenshots/full_model_eager_view.png)\n",
    "*Full Eager Model - Pytorch Profiler trace*\n",
    "\n",
    "- Sparse Arch is now not the biggest piece of the pie\n",
    "- PredictionLayer is the highest\n",
    "    - Top MLP and sigmoid  \n",
    "\n",
    "### `torch.compile` view\n",
    "\n",
    "![Full Model Torch Compiled](./perf_screenshots/full_model_torch_compiled.png)\n",
    "*Full `torch.compile` Model - Pytorch Profiler trace*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19abe6a3-74fa-47b8-b439-1b4af946ffa0",
   "metadata": {},
   "source": [
    "# torch.compile -> triton code generation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b529a0-b65a-48aa-8ec9-dd58064aabc9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generate triton code\n",
    "\n",
    "> TORCH_LOGS=output_code CUDA_LAUNCH_BLOCKING=1 python model_train.py\n",
    "\n",
    "## Inspect\n",
    "\n",
    "- Prints generated code for you\n",
    "- Should see `... torch._inductor.graph.__output_code: [INFO] Output code written to: ...`\n",
    "- Shows source nodes from where the code was generated\n",
    "- Fused kernels:\n",
    "    - fused_relu\n",
    "    - fused_cat\n",
    "    - fused_embedding\n",
    "    - fused_sigmoid_squeeze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87661f85-04e4-4649-acb4-4a19061d8f05",
   "metadata": {},
   "source": [
    "## Write our own\n",
    "\n",
    "### Kernel\n",
    "\n",
    "```python\n",
    "@triton.jit\n",
    "def pointwise_add_relu_fusion_512(in_out_ptr0, in_ptr0, XBLOCK : tl.constexpr):\n",
    "    # Number of elements in in_out_ptr0 (B X N)\n",
    "    xnumel = 65536\n",
    "    # This program will process inputs that are offset from the initial data.\n",
    "    # For instance, if you had a strided tensor of 65536 i.e. 128 X 512 and XBLOCK = 512\n",
    "    # the programs will each access the elements [0:512, 512:1024, ...].\n",
    "    # i.e. offsets is a list of pointers:\n",
    "    # Question: Can you see how torch.compile is allocating blocks here? \n",
    "    # below we will call this N = 512\n",
    "    xoffset = tl.program_id(0) * XBLOCK\n",
    "    # block threads\n",
    "    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n",
    "    # masks to guard against overflow\n",
    "    xmask = xindex < xnumel\n",
    "    # xindex will have elements from 0:N, N:2N where N = dense @ weights\n",
    "    x2 = xindex\n",
    "    # bias i.e. 1D tensor with only N elements\n",
    "    # mod will give the us the right \n",
    "    x0 = xindex % 512\n",
    "    # load the N elements\n",
    "    tmp0 = tl.load(in_out_ptr0 + (x2), xmask)\n",
    "    # load the 1D tensor\n",
    "    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')\n",
    "    # result = bias + dense @ weights\n",
    "    tmp2 = tmp0 + tmp1\n",
    "    # relu: can also use tl.maximum\n",
    "    tmp3 = triton_helpers.maximum(0, tmp2) \n",
    "    # output moved over\n",
    "    tl.store(in_out_ptr0 + (x2), tmp3, None)\n",
    "```\n",
    "\n",
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0f9d6ad-c572-44da-944a-8e11a875d477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], device='cuda:0')\n",
      "tensor([1., 1., 1.], device='cuda:0')\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]], device='cuda:0')\n",
      "tensor([[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "        [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "        [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "        ...,\n",
      "        [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "        [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "        [2., 2., 2.,  ..., 2., 2., 2.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import triton\n",
    "import torch\n",
    "import triton.language as tl\n",
    "from torch._inductor import triton_helpers\n",
    "from torch._inductor.triton_heuristics import grid\n",
    "\n",
    "@triton.jit\n",
    "def pointwise_add_relu_fusion_512(in_out_ptr0, in_ptr0, XBLOCK : tl.constexpr):\n",
    "    xnumel = 65536\n",
    "    xoffset = tl.program_id(0) * XBLOCK\n",
    "    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n",
    "    xmask = xindex < xnumel\n",
    "    # dense @ weights\n",
    "    x2 = xindex\n",
    "    # bias\n",
    "    x0 = xindex % 512\n",
    "    tmp0 = tl.load(in_out_ptr0 + (x2), xmask)\n",
    "    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')\n",
    "    # bias + dense @ weights\n",
    "    tmp2 = tmp0 + tmp1\n",
    "    tmp3 = triton_helpers.maximum(0, tmp2)\n",
    "    tl.store(in_out_ptr0 + (x2), tmp3, None)\n",
    "\n",
    "\n",
    "torch.cuda.set_device(0)  # no-op to ensure context\n",
    "X = torch.ones(size=(128, 512), device='cuda')\n",
    "print(X[:3, :3])\n",
    "Y = torch.ones(size=(512,), device='cuda')\n",
    "print(Y[:3])\n",
    "eager_result = torch.maximum(X + Y, torch.tensor(0., device='cuda'))\n",
    "print(eager_result[:3, :3])\n",
    "pointwise_add_relu_fusion_512[grid(65536)](X, Y, 512)\n",
    "print(X)\n",
    "torch.testing.assert_close(X, eager_result, rtol=1e-4, atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a42047-da22-4496-ad93-0b890d5851d0",
   "metadata": {},
   "source": [
    "## Cuda Kernel\n",
    "\n",
    "### Ask ChatGPT to generate the kernel for us\n",
    "\n",
    "![Chat GPT Input](./data/chatgpt_input.png)\n",
    "\n",
    "### ChatGPT output (without any changes)\n",
    "\n",
    "```cpp\n",
    "#include <cuda_fp16.h>\n",
    "\n",
    "__global__ void pointwise_add_relu_fusion_512(float* in_out_ptr0, const float* in_ptr0, const int XBLOCK) {\n",
    "    const int xnumel = 65536;\n",
    "    const int N = 512; // Value of N from the Triton kernel\n",
    "    const int tid = threadIdx.x;\n",
    "    const int xoffset = blockIdx.x * XBLOCK;\n",
    "    const int xindex = xoffset + tid;\n",
    "    const bool xmask = xindex < xnumel;\n",
    "    \n",
    "    if (xmask) {\n",
    "        int x2 = xindex;\n",
    "        int x0 = xindex % N;\n",
    "        \n",
    "        float tmp0 = in_out_ptr0[x2];\n",
    "        float tmp1 = in_ptr0[x0];\n",
    "        float tmp2 = tmp0 + tmp1;\n",
    "        float tmp3 = max(0.0f, tmp2); // ReLU operation\n",
    "        \n",
    "        in_out_ptr0[x2] = tmp3;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### Let's run the generated CUDA kernel\n",
    "\n",
    "> NOTE: To run torch native, you can download it as below or add conda environment to $CMAKE_PREFIX_PATH\n",
    "\n",
    "```\n",
    "wget https://download.pytorch.org/libtorch/cu121/libtorch-cxx11-abi-shared-with-deps-2.2.1%2Bcu121.zip # Download torch native lib\n",
    "```\n",
    "\n",
    "### Build the cmake project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f371d14-f084-4826-b750-f35ce40afe9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- CMake version: 3.22.1\n",
      "-- Caffe2: CUDA detected: 12.1\n",
      "-- Caffe2: CUDA nvcc is: /home/ksharma/anaconda3/envs/cuda-learn/bin/nvcc\n",
      "-- Caffe2: CUDA toolkit directory: /home/ksharma/anaconda3/envs/cuda-learn\n",
      "-- Caffe2: Header version is: 12.1\n",
      "-- /home/ksharma/anaconda3/envs/cuda-learn/lib/libnvrtc.so shorthash is c993a6f1\n",
      "-- USE_CUDNN is set to 0. Compiling without cuDNN support\n",
      "-- USE_CUSPARSELT is set to 0. Compiling without cuSPARSELt support\n",
      "-- Autodetected CUDA architecture(s):  7.5\n",
      "-- Added CUDA NVCC flags for: -gencode;arch=compute_75,code=sm_75\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /home/ksharma/dev/git/cuda-mode-lecture/kernels/cmake-build-debug\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target pointwise_add_relu_fused\u001b[0m\n",
      "[100%] Built target pointwise_add_relu_fused\n"
     ]
    }
   ],
   "source": [
    "! mkdir -p kernels/cmake-build-debug && cd kernels/cmake-build-debug && cmake build .. && make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ef7bde3-d0b8-435b-898b-d5baa009105d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor x:\n",
      "-0.9247 -0.4253 -2.6438  0.1452 -0.1209 -0.5797 -0.6229 -0.3284 -1.0745 -0.3631\n",
      "-1.6711  2.2655  0.3117 -0.1842  1.2866  1.1820 -0.1271  1.2169  1.4353  1.0605\n",
      "-0.4941 -1.4244 -0.7244 -1.2973  0.0697 -0.0074  1.8969  0.6878 -0.0779 -0.8373\n",
      " 1.3506 -0.2879 -0.5965 -0.3283 -0.9086 -0.8059 -0.7407 -0.0504  0.5435  1.5150\n",
      " 0.0141  0.4532  1.6349  0.7124 -0.1806  1.0252 -1.4622 -0.7554 -0.1836  0.3824\n",
      " 0.3918 -0.0830  0.8971 -1.1123  0.1116  0.4863 -0.5499 -0.3231 -0.5469  0.9049\n",
      " 0.2837  0.1210  0.4730 -1.0823 -0.0334 -0.9734  0.9559 -1.1795 -1.0064  0.1160\n",
      " 0.6852 -0.4124 -0.6738 -0.5404  0.6898 -1.5517  0.3805 -0.0436  0.3597 -0.5043\n",
      "[ CUDAFloatType{8,10} ]\n",
      "Tensor y:\n",
      " 0.1808\n",
      "-0.5523\n",
      " 0.9238\n",
      "-0.7350\n",
      " 1.3800\n",
      " 0.8676\n",
      " 0.1297\n",
      "-0.9406\n",
      " 0.8109\n",
      " 0.8821\n",
      "[ CUDAFloatType{10} ]\n",
      "Expected:\n",
      " 0.0000  0.0000  0.0000  0.0000  1.2591  0.2879  0.0000  0.0000  0.0000  0.5189\n",
      " 0.0000  1.7132  1.2355  0.0000  2.6666  2.0496  0.0026  0.2763  2.2462  1.9425\n",
      " 0.0000  0.0000  0.1994  0.0000  1.4497  0.8602  2.0266  0.0000  0.7330  0.0448\n",
      " 1.5315  0.0000  0.3273  0.0000  0.4714  0.0617  0.0000  0.0000  1.3544  2.3971\n",
      " 0.1949  0.0000  2.5587  0.0000  1.1994  1.8929  0.0000  0.0000  0.6273  1.2644\n",
      " 0.5726  0.0000  1.8209  0.0000  1.4916  1.3539  0.0000  0.0000  0.2640  1.7869\n",
      " 0.4645  0.0000  1.3968  0.0000  1.3465  0.0000  1.0856  0.0000  0.0000  0.9980\n",
      " 0.8660  0.0000  0.2500  0.0000  2.0698  0.0000  0.5102  0.0000  1.1706  0.3778\n",
      "[ CUDAFloatType{8,10} ]\n",
      "Result:\n",
      " 0.0000  0.0000  0.0000  0.0000  1.2591  0.2879  0.0000  0.0000  0.0000  0.5189\n",
      " 0.0000  1.7132  1.2355  0.0000  2.6666  2.0496  0.0026  0.2763  2.2462  1.9425\n",
      " 0.0000  0.0000  0.1994  0.0000  1.4497  0.8602  2.0266  0.0000  0.7330  0.0448\n",
      " 1.5315  0.0000  0.3273  0.0000  0.4714  0.0617  0.0000  0.0000  1.3544  2.3971\n",
      " 0.1949  0.0000  2.5587  0.0000  1.1994  1.8929  0.0000  0.0000  0.6273  1.2644\n",
      " 0.5726  0.0000  1.8209  0.0000  1.4916  1.3539  0.0000  0.0000  0.2640  1.7869\n",
      " 0.4645  0.0000  1.3968  0.0000  1.3465  0.0000  1.0856  0.0000  0.0000  0.9980\n",
      " 0.8660  0.0000  0.2500  0.0000  2.0698  0.0000  0.5102  0.0000  1.1706  0.3778\n",
      "[ CUDAFloatType{8,10} ]\n",
      "All Match: true\n"
     ]
    }
   ],
   "source": [
    "!./kernels/cmake-build-debug/pointwise_add_relu_fused"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9afaf0-576a-4a57-b9ac-91ea779d75f4",
   "metadata": {},
   "source": [
    "### (OR) Run it locally with pytorch utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81ca18d8-86c9-48aa-801f-b48a14bb9613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "__global__ void add_relu_fusion_kernel(float* in_out_ptr0, const float* in_ptr0, const int xnumel ,const int XBLOCK) {\n",
      "    const int tid = threadIdx.x;\n",
      "    const int xoffset = blockIdx.x * XBLOCK;\n",
      "    const int xindex = xoffset + tid;\n",
      "    const bool xmask = xindex < xnumel;\n",
      "\n",
      "    if (xmask) {\n",
      "        int x2 = xindex;\n",
      "        int x0 = xindex % XBLOCK;\n",
      "        float tmp0 = in_out_ptr0[x2];\n",
      "        float tmp1 = in_ptr0[x0];\n",
      "        float tmp2 = tmp0 + tmp1;\n",
      "        float tmp3 = max(0.0f, tmp2); // ReLU operation\n",
      "\n",
      "        in_out_ptr0[x2] = tmp3;\n",
      "    }\n",
      "}\n",
      "\n",
      "torch::Tensor add_relu_fusion(torch::Tensor in_out, const torch::Tensor& in) {\n",
      "    auto sizes = in_out.sizes();\n",
      "    auto XBLOCK = sizes[1];\n",
      "    auto numel = in_out.numel();\n",
      "    dim3 threadsPerBlock(XBLOCK);\n",
      "    dim3 numBlocks((numel + XBLOCK - 1) / XBLOCK);\n",
      "    add_relu_fusion_kernel<<<numBlocks, threadsPerBlock>>>(in_out.data_ptr<float>(), in.data_ptr<float>(), numel, XBLOCK);\n",
      "    cudaDeviceSynchronize();\n",
      "    return std::move(in_out);\n",
      "}\n",
      "----\n",
      "\n",
      "torch::Tensor add_relu_fusion(torch::Tensor in_out, const torch::Tensor& in);\n"
     ]
    }
   ],
   "source": [
    "cuda_code_file = \"./kernels/src/pointwise_add_relu_fused.cu\"\n",
    "header_code_file = \"./kernels/src/pointwise_add_relu_fused.cuh\"\n",
    "\n",
    "with open(cuda_code_file) as f:\n",
    "    cuda_code = \"\".join([f for f in f.readlines() if not f.startswith(\"#include\")])\n",
    "    print(cuda_code)\n",
    "\n",
    "print(\"----\")\n",
    "\n",
    "with open(header_code_file) as f:\n",
    "    header_code = \"\".join([f for f in f.readlines() if not f.startswith(\"#include\")])\n",
    "    print(header_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed787f6f-23eb-4622-bdb2-4ae99f5ceabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘./build’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ./build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87c2998a-f14a-4d8f-9eaa-554169c33e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file ./build/build.ninja...\n",
      "Building extension module kernel_extension...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module kernel_extension...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.cpp_extension import load_inline\n",
    "\n",
    "cuda_extension = load_inline(\n",
    "    name='kernel_extension',\n",
    "    cpp_sources=header_code,\n",
    "    cuda_sources=cuda_code,\n",
    "    functions=[\"add_relu_fusion\"],\n",
    "    with_cuda=True,\n",
    "    verbose=True,\n",
    "    extra_cuda_cflags=[\"-O2\"],\n",
    "    build_directory='./build',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4674f5f-5132-4703-ae1d-af28ef8f8b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'add_relu_fusion']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(cuda_extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2420a7ce-8ae6-4b09-8315-a58a8d9a6cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "        [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "        [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "        ...,\n",
      "        [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "        [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "        [2., 2., 2.,  ..., 2., 2., 2.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(0)  # no-op to ensure context\n",
    "X = torch.ones(size=(128, 512), device='cuda')\n",
    "Y = torch.ones(size=(512,), device='cuda')\n",
    "cuda_extension.add_relu_fusion(X, Y)\n",
    "print(X)\n",
    "torch.testing.assert_close(X, eager_result, rtol=1e-4, atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2e4bf2-7f21-47e7-b4a1-0bceedffcacf",
   "metadata": {},
   "source": [
    "# LoRA Fused Kernels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08469ba-5059-42fe-9925-8c8b32d85475",
   "metadata": {},
   "source": [
    "## LoRA (LOW-RANK ADAPTATION)\n",
    "\n",
    "<img src=\"./data/lora.png\" width=\"400\"/>\n",
    "\n",
    "Source: https://arxiv.org/pdf/2106.09685"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d834683-f948-4467-af45-868ca6e85378",
   "metadata": {},
   "source": [
    "## Fused Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9793c2d6-ab24-4604-8627-e23c4020536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lora_on_simple_mlp import *\n",
    "from kernels.triton_fused_add_mul_relu import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1f211b-3246-4c73-b967-7f33fdaecc66",
   "metadata": {},
   "source": [
    "### Fused Mul Add Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab4fbb17-3077-42a1-aa84-1430d8681d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "Input tensor([[-0.3104, -0.0343,  0.1756, -2.2804,  0.5039,  0.5596, -0.0750,  0.9691],\n",
      "        [-0.2357, -0.4582,  0.5661,  1.2851, -1.8667, -0.0312,  1.2433,  1.3689],\n",
      "        [-1.0753, -0.0158, -1.4481, -1.3089,  0.6980, -0.3300, -0.7708, -0.4946],\n",
      "        [ 3.1702,  0.0387, -1.5728,  1.2985, -0.4419, -0.6965, -1.4002, -0.0884],\n",
      "        [-0.2369, -0.5956, -0.0263,  0.0546, -0.7082,  0.0642,  1.2830, -1.8728],\n",
      "        [ 0.7562, -0.6345,  1.1176,  1.3382,  0.1994,  0.0671, -0.4159,  0.1468],\n",
      "        [-0.2672, -0.7882,  0.5857, -1.0649,  1.0950,  0.2490, -0.3271, -0.8691],\n",
      "        [ 0.5576, -0.1883, -0.5894, -1.0192, -1.4553, -0.8599, -1.3645, -0.0069]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "Expected Output tensor([[1.3509, 0.0000, 0.3476, 0.0000, 0.0000, 1.6216, 1.0154, 1.7055],\n",
      "        [0.7115, 0.0000, 0.4937, 2.3202, 0.0000, 0.0000, 2.9328, 1.9187],\n",
      "        [0.3889, 0.0000, 0.0000, 0.5725, 0.0000, 0.1259, 0.0000, 0.3423],\n",
      "        [4.9722, 0.0000, 0.0000, 2.0715, 0.0000, 0.0000, 0.0000, 0.3307],\n",
      "        [1.3148, 0.0000, 0.4223, 1.3483, 0.0000, 0.0000, 2.0066, 0.0000],\n",
      "        [1.8233, 0.0000, 1.0833, 2.2275, 0.4882, 0.3238, 0.4521, 0.8122],\n",
      "        [1.1539, 0.0000, 0.3214, 0.0000, 0.0088, 0.0000, 0.2131, 0.1656],\n",
      "        [2.3094, 0.0000, 0.0000, 0.2560, 0.0000, 0.0000, 0.0000, 0.8519]],\n",
      "       device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(triton.__version__)\n",
    "in_out_tensor, in_tensor, bias = get_inputs(add_manual_size=True)\n",
    "expected_output = torch.maximum(in_out_tensor + 0.5 * in_tensor + bias, torch.tensor(0., device='cuda'))\n",
    "print(\"Input\", in_out_tensor)\n",
    "print(\"Expected Output\", expected_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23383ef8-996c-4d3f-b460-8ffd6fe942f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 1 tensor([[1.3509, 0.0000, 0.3476, 0.0000, 0.0000, 1.6216, 1.0154, 1.7055],\n",
      "        [0.7115, 0.0000, 0.4937, 2.3202, 0.0000, 0.0000, 2.9328, 1.9187],\n",
      "        [0.3889, 0.0000, 0.0000, 0.5725, 0.0000, 0.1259, 0.0000, 0.3423],\n",
      "        [4.9722, 0.0000, 0.0000, 2.0715, 0.0000, 0.0000, 0.0000, 0.3307],\n",
      "        [1.3148, 0.0000, 0.4223, 1.3483, 0.0000, 0.0000, 2.0066, 0.0000],\n",
      "        [1.8233, 0.0000, 1.0833, 2.2275, 0.4882, 0.3238, 0.4521, 0.8122],\n",
      "        [1.1539, 0.0000, 0.3214, 0.0000, 0.0088, 0.0000, 0.2131, 0.1656],\n",
      "        [2.3094, 0.0000, 0.0000, 0.2560, 0.0000, 0.0000, 0.0000, 0.8519]],\n",
      "       device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "BLOCK_SIZE = 8\n",
    "grid = lambda meta: (triton.cdiv(in_out_tensor.numel(), meta['BLOCK_SIZE']),)\n",
    "fused_add_mul_relu[grid](in_out_tensor, bias, in_tensor, in_out_tensor.numel(), BLOCK_SIZE=BLOCK_SIZE)\n",
    "print(\"Output 1\", in_out_tensor)\n",
    "torch.testing.assert_close(in_out_tensor, expected_output, rtol=1e-4, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "895f27e7-bf5f-4aac-908b-9c4351f7a9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 2 tensor([[1.3509, 0.0000, 0.3476, 0.0000, 0.0000, 1.6216, 1.0154, 1.7055],\n",
      "        [0.7115, 0.0000, 0.4937, 2.3202, 0.0000, 0.0000, 2.9328, 1.9187],\n",
      "        [0.3889, 0.0000, 0.0000, 0.5725, 0.0000, 0.1259, 0.0000, 0.3423],\n",
      "        [4.9722, 0.0000, 0.0000, 2.0715, 0.0000, 0.0000, 0.0000, 0.3307],\n",
      "        [1.3148, 0.0000, 0.4223, 1.3483, 0.0000, 0.0000, 2.0066, 0.0000],\n",
      "        [1.8233, 0.0000, 1.0833, 2.2275, 0.4882, 0.3238, 0.4521, 0.8122],\n",
      "        [1.1539, 0.0000, 0.3214, 0.0000, 0.0088, 0.0000, 0.2131, 0.1656],\n",
      "        [2.3094, 0.0000, 0.0000, 0.2560, 0.0000, 0.0000, 0.0000, 0.8519]],\n",
      "       device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "in_out_tensor, in_tensor, bias = get_inputs(add_manual_size=True)\n",
    "num_weights = bias.numel()\n",
    "fused_add_mul_relu_cleaner[grid](in_out_tensor, bias, in_tensor, num_weights, in_out_tensor.numel(), multiplier=0.5,\n",
    "                                 BLOCK_SIZE=BLOCK_SIZE)\n",
    "print(\"Output 2\", in_out_tensor)\n",
    "torch.testing.assert_close(in_out_tensor, expected_output, rtol=1e-4, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf24a03d-1d27-408e-8b94-f37f72b5cca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
